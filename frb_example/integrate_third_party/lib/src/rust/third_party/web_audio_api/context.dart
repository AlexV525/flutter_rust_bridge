// This file is automatically generated, so please do not edit it.
// Generated by `flutter_rust_bridge`@ 2.0.0-dev.37.

// ignore_for_file: invalid_use_of_internal_member, unused_import, unnecessary_import

import '../../frb_generated.dart';
import '../web_audio_api.dart';
import 'media_streams.dart';
import 'node.dart';
import 'package:flutter_rust_bridge/flutter_rust_bridge_for_generated.dart';
import 'package:freezed_annotation/freezed_annotation.dart' hide protected;
part 'context.freezed.dart';

// These functions are ignored because they are not marked as `pub`: `address`, `calculate_suspend_frame`, `clear_event_handler`, `connect_listener_to_panner`, `connect`, `context`, `current_time`, `destination_channel_config`, `disconnect`, `ensure_audio_listener_present`, `get`, `id`, `is_valid_sink_id`, `listener`, `lock_control_msg_sender`, `mark_node_dropped`, `max_channel_count`, `new`, `new`, `offline`, `post_message`, `queue_audio_param_connect`, `register`, `resolve_queued_control_msgs`, `sample_rate`, `send_control_msg`, `send_event`, `set_event_handler`, `set_state`, `state`
// These functions are ignored because they have generic arguments: `decode_audio_data_sync`, `decode_audio_data_sync`, `decode_audio_data_sync`, `run_diagnostics`, `set_oncomplete`, `set_onsinkchange`, `set_onstatechange`, `set_onstatechange`, `set_onstatechange`, `suspend_sync`
// These types are ignored because they are not used by any `pub` functions: `AudioNodeIdProvider`, `AudioNodeId`, `ConcreteBaseAudioContextInner`, `OfflineAudioContextRenderer`
// These functions are ignored: `create_media_element_source`, `resume`, `set_sink_id_sync`

// Rust type: RustOpaqueMoi<flutter_rust_bridge::for_generated::RustAutoOpaqueInner<AudioContext>>
@sealed
class AudioContext extends RustOpaque {
  // Not to be used by end users
  AudioContext.frbInternalDcoDecode(List<dynamic> wire)
      : super.frbInternalDcoDecode(wire, _kStaticData);

  // Not to be used by end users
  AudioContext.frbInternalSseDecode(BigInt ptr, int externalSizeOnNative)
      : super.frbInternalSseDecode(ptr, externalSizeOnNative, _kStaticData);

  static final _kStaticData = RustArcStaticData(
    rustArcIncrementStrongCount:
        RustLib.instance.api.rust_arc_increment_strong_count_AudioContext,
    rustArcDecrementStrongCount:
        RustLib.instance.api.rust_arc_decrement_strong_count_AudioContext,
    rustArcDecrementStrongCountPtr:
        RustLib.instance.api.rust_arc_decrement_strong_count_AudioContextPtr,
  );

  Future<AudioBuffer> decodeAudioDataSync({required String inputPath}) =>
      RustLib.instance.api
          .crateApiOverrideWebAudioApiAudioContextFrbOverrideDecodeAudioDataSync(
              that: this, inputPath: inputPath);

  Future<String> outputLatency() => RustLib.instance.api
          .crateApiOverrideWebAudioApiAudioContextFrbOverrideOutputLatency(
        that: this,
      );

  Future<void> base() =>
      RustLib.instance.api.webAudioApiContextAudioContextBase(
        that: this,
      );

  /// This represents the number of seconds of processing latency incurred by
  /// the `AudioContext` passing the audio from the `AudioDestinationNode`
  /// to the audio subsystem.
  Future<double> baseLatency() =>
      RustLib.instance.api.webAudioApiContextAudioContextBaseLatency(
        that: this,
      );

  /// Unset the callback to run when the audio sink has changed
  Future<void> clearOnsinkchange() =>
      RustLib.instance.api.webAudioApiContextAudioContextClearOnsinkchange(
        that: this,
      );

  /// Unset the callback to run when the state of the AudioContext has changed
  Future<void> clearOnstatechange() =>
      RustLib.instance.api.webAudioApiContextAudioContextClearOnstatechange(
        that: this,
      );

  /// Closes the `AudioContext`, releasing the system resources being used.
  ///
  /// This will not automatically release all `AudioContext`-created objects, but will suspend
  /// the progression of the currentTime, and stop processing audio data.
  ///
  /// # Panics
  ///
  /// Will panic when this function is called multiple times
  Future<void> close() =>
      RustLib.instance.api.webAudioApiContextAudioContextClose(
        that: this,
      );

  /// Closes the `AudioContext`, releasing the system resources being used.
  ///
  /// This will not automatically release all `AudioContext`-created objects, but will suspend
  /// the progression of the currentTime, and stop processing audio data.
  ///
  /// This function operates synchronously and blocks the current thread until the audio thread
  /// has stopped processing.
  ///
  /// # Panics
  ///
  /// Will panic when this function is called multiple times
  Future<void> closeSync() =>
      RustLib.instance.api.webAudioApiContextAudioContextCloseSync(
        that: this,
      );

  /// Creates a `AnalyserNode`
  Future<AnalyserNode> createAnalyser() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateAnalyser(
        that: this,
      );

  /// Create an `AudioParam`.
  ///
  /// Call this inside the `register` closure when setting up your `AudioNode`
  Future<(AudioParam, AudioParamId)> createAudioParam(
          {required AudioParamDescriptor opts,
          required AudioContextRegistration dest}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateAudioParam(
          that: this, opts: opts, dest: dest);

  /// Creates an `BiquadFilterNode` which implements a second order filter
  Future<BiquadFilterNode> createBiquadFilter() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateBiquadFilter(
        that: this,
      );

  /// Create an new "in-memory" `AudioBuffer` with the given number of channels,
  /// length (i.e. number of samples per channel) and sample rate.
  ///
  /// Note: In most cases you will want the sample rate to match the current
  /// audio context sample rate.
  Future<AudioBuffer> createBuffer(
          {required BigInt numberOfChannels,
          required BigInt length,
          required double sampleRate}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateBuffer(
          that: this,
          numberOfChannels: numberOfChannels,
          length: length,
          sampleRate: sampleRate);

  /// Creates an `AudioBufferSourceNode`
  Future<AudioBufferSourceNode> createBufferSource() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateBufferSource(
        that: this,
      );

  /// Creates a `ChannelMergerNode`
  Future<ChannelMergerNode> createChannelMerger(
          {required BigInt numberOfInputs}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateChannelMerger(
          that: this, numberOfInputs: numberOfInputs);

  /// Creates a `ChannelSplitterNode`
  Future<ChannelSplitterNode> createChannelSplitter(
          {required BigInt numberOfOutputs}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateChannelSplitter(
          that: this, numberOfOutputs: numberOfOutputs);

  /// Creates an `ConstantSourceNode`, a source representing a constant value
  Future<ConstantSourceNode> createConstantSource() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateConstantSource(
        that: this,
      );

  /// Creates an `ConvolverNode`, a processing node which applies linear convolution
  Future<ConvolverNode> createConvolver() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateConvolver(
        that: this,
      );

  /// Creates a `DelayNode`, delaying the audio signal
  Future<DelayNode> createDelay({required double maxDelayTime}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateDelay(
          that: this, maxDelayTime: maxDelayTime);

  /// Creates a `DynamicsCompressorNode`, compressing the audio signal
  Future<DynamicsCompressorNode> createDynamicsCompressor() =>
      RustLib.instance.api
          .webAudioApiContextAudioContextCreateDynamicsCompressor(
        that: this,
      );

  /// Creates an `GainNode`, to control audio volume
  Future<GainNode> createGain() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateGain(
        that: this,
      );

  /// Creates an `IirFilterNode`
  ///
  /// # Arguments
  ///
  /// * `feedforward` - An array of the feedforward (numerator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  /// * `feedback` - An array of the feedback (denominator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  Future<IirFilterNode> createIirFilter(
          {required List<double> feedforward,
          required List<double> feedback}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateIirFilter(
          that: this, feedforward: feedforward, feedback: feedback);

  /// Creates a [`MediaStreamAudioDestinationNode`](node::MediaStreamAudioDestinationNode)
  Future<MediaStreamAudioDestinationNode> createMediaStreamDestination() =>
      RustLib.instance.api
          .webAudioApiContextAudioContextCreateMediaStreamDestination(
        that: this,
      );

  /// Creates a [`MediaStreamAudioSourceNode`](node::MediaStreamAudioSourceNode) from a
  /// [`MediaStream`]
  Future<MediaStreamAudioSourceNode> createMediaStreamSource(
          {required MediaStream media}) =>
      RustLib.instance.api
          .webAudioApiContextAudioContextCreateMediaStreamSource(
              that: this, media: media);

  /// Creates a [`MediaStreamTrackAudioSourceNode`](node::MediaStreamTrackAudioSourceNode) from a
  /// [`MediaStreamTrack`]
  Future<MediaStreamTrackAudioSourceNode> createMediaStreamTrackSource(
          {required MediaStreamTrack media}) =>
      RustLib.instance.api
          .webAudioApiContextAudioContextCreateMediaStreamTrackSource(
              that: this, media: media);

  /// Creates an `OscillatorNode`, a source representing a periodic waveform.
  Future<OscillatorNode> createOscillator() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateOscillator(
        that: this,
      );

  /// Creates a `PannerNode`
  Future<PannerNode> createPanner() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreatePanner(
        that: this,
      );

  /// Creates a periodic wave
  ///
  /// Please note that this constructor deviates slightly from the spec by requiring a single
  /// argument with the periodic wave options.
  Future<PeriodicWave> createPeriodicWave(
          {required PeriodicWaveOptions options}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreatePeriodicWave(
          that: this, options: options);

  /// Creates an `ScriptProcessorNode` for custom audio processing (deprecated);
  ///
  /// # Panics
  ///
  /// This function panics if:
  /// - `buffer_size` is not 256, 512, 1024, 2048, 4096, 8192, or 16384
  /// - the number of input and output channels are both zero
  /// - either of the channel counts exceed [`crate::MAX_CHANNELS`]
  Future<ScriptProcessorNode> createScriptProcessor(
          {required BigInt bufferSize,
          required BigInt numberOfInputChannels,
          required BigInt numberOfOutputChannels}) =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateScriptProcessor(
          that: this,
          bufferSize: bufferSize,
          numberOfInputChannels: numberOfInputChannels,
          numberOfOutputChannels: numberOfOutputChannels);

  /// Creates an `StereoPannerNode` to pan a stereo output
  Future<StereoPannerNode> createStereoPanner() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateStereoPanner(
        that: this,
      );

  /// Creates a `WaveShaperNode`
  Future<WaveShaperNode> createWaveShaper() =>
      RustLib.instance.api.webAudioApiContextAudioContextCreateWaveShaper(
        that: this,
      );

  /// This is the time in seconds of the sample frame immediately following the last sample-frame
  /// in the block of audio most recently processed by the context’s rendering graph.
  Future<double> currentTime() =>
      RustLib.instance.api.webAudioApiContextAudioContextCurrentTime(
        that: this,
      );

  /// Returns an `AudioDestinationNode` representing the final destination of all audio in the
  /// context. It can be thought of as the audio-rendering device.
  Future<AudioDestinationNode> destination() =>
      RustLib.instance.api.webAudioApiContextAudioContextDestination(
        that: this,
      );

  /// Returns the `AudioListener` which is used for 3D spatialization
  Future<AudioListener> listener() =>
      RustLib.instance.api.webAudioApiContextAudioContextListener(
        that: this,
      );

  /// Creates and returns a new `AudioContext` object.
  ///
  /// This will play live audio on the default output device.
  ///
  /// ```no_run
  /// use web_audio_api::context::{AudioContext, AudioContextOptions};
  ///
  /// // Request a sample rate of 44.1 kHz and default latency (buffer size 128, if available)
  /// let opts = AudioContextOptions {
  ///     sample_rate: Some(44100.),
  ///     ..AudioContextOptions::default()
  /// };
  ///
  /// // Setup the audio context that will emit to your speakers
  /// let context = AudioContext::new(opts);
  ///
  /// // Alternatively, use the default constructor to get the best settings for your hardware
  /// // let context = AudioContext::default();
  /// ```
  ///
  /// # Panics
  ///
  /// The `AudioContext` constructor will panic when an invalid `sinkId` is provided in the
  /// `AudioContextOptions`. In a future version, a `try_new` constructor will be introduced that
  /// never panics.
  factory AudioContext({required AudioContextOptions options}) =>
      RustLib.instance.api.webAudioApiContextAudioContextNew(options: options);

  /// Returns an [`AudioRenderCapacity`] instance associated with an AudioContext.
  Future<void> renderCapacity() =>
      RustLib.instance.api.webAudioApiContextAudioContextRenderCapacity(
        that: this,
      );

  /// Resumes the progression of time in an audio context that has previously been
  /// suspended/paused.
  ///
  /// This function operates synchronously and blocks the current thread until the audio thread
  /// has started processing again.
  ///
  /// # Panics
  ///
  /// Will panic if:
  ///
  /// * The audio device is not available
  /// * For a `BackendSpecificError`
  Future<void> resumeSync() =>
      RustLib.instance.api.webAudioApiContextAudioContextResumeSync(
        that: this,
      );

  /// The sample rate (in sample-frames per second) at which the `AudioContext` handles audio.
  Future<double> sampleRate() =>
      RustLib.instance.api.webAudioApiContextAudioContextSampleRate(
        that: this,
      );

  /// Identifier or the information of the current audio output device.
  ///
  /// The initial value is `""`, which means the default audio output device.
  Future<String> sinkId() =>
      RustLib.instance.api.webAudioApiContextAudioContextSinkId(
        that: this,
      );

  /// Returns state of current context
  Future<AudioContextState> state() =>
      RustLib.instance.api.webAudioApiContextAudioContextState(
        that: this,
      );

  /// Suspends the progression of time in the audio context.
  ///
  /// This will temporarily halt audio hardware access and reducing CPU/battery usage in the
  /// process.
  ///
  /// # Panics
  ///
  /// Will panic if:
  ///
  /// * The audio device is not available
  /// * For a `BackendSpecificError`
  Future<void> suspend() =>
      RustLib.instance.api.webAudioApiContextAudioContextSuspend(
        that: this,
      );

  /// Suspends the progression of time in the audio context.
  ///
  /// This will temporarily halt audio hardware access and reducing CPU/battery usage in the
  /// process.
  ///
  /// This function operates synchronously and blocks the current thread until the audio thread
  /// has stopped processing.
  ///
  /// # Panics
  ///
  /// Will panic if:
  ///
  /// * The audio device is not available
  /// * For a `BackendSpecificError`
  Future<void> suspendSync() =>
      RustLib.instance.api.webAudioApiContextAudioContextSuspendSync(
        that: this,
      );
}

// Rust type: RustOpaqueMoi<flutter_rust_bridge::for_generated::RustAutoOpaqueInner<AudioParamId>>
@sealed
class AudioParamId extends RustOpaque {
  // Not to be used by end users
  AudioParamId.frbInternalDcoDecode(List<dynamic> wire)
      : super.frbInternalDcoDecode(wire, _kStaticData);

  // Not to be used by end users
  AudioParamId.frbInternalSseDecode(BigInt ptr, int externalSizeOnNative)
      : super.frbInternalSseDecode(ptr, externalSizeOnNative, _kStaticData);

  static final _kStaticData = RustArcStaticData(
    rustArcIncrementStrongCount:
        RustLib.instance.api.rust_arc_increment_strong_count_AudioParamId,
    rustArcDecrementStrongCount:
        RustLib.instance.api.rust_arc_decrement_strong_count_AudioParamId,
    rustArcDecrementStrongCountPtr:
        RustLib.instance.api.rust_arc_decrement_strong_count_AudioParamIdPtr,
  );
}

// Rust type: RustOpaqueMoi<flutter_rust_bridge::for_generated::RustAutoOpaqueInner<ConcreteBaseAudioContext>>
@sealed
class ConcreteBaseAudioContext extends RustOpaque {
  // Not to be used by end users
  ConcreteBaseAudioContext.frbInternalDcoDecode(List<dynamic> wire)
      : super.frbInternalDcoDecode(wire, _kStaticData);

  // Not to be used by end users
  ConcreteBaseAudioContext.frbInternalSseDecode(
      BigInt ptr, int externalSizeOnNative)
      : super.frbInternalSseDecode(ptr, externalSizeOnNative, _kStaticData);

  static final _kStaticData = RustArcStaticData(
    rustArcIncrementStrongCount: RustLib
        .instance.api.rust_arc_increment_strong_count_ConcreteBaseAudioContext,
    rustArcDecrementStrongCount: RustLib
        .instance.api.rust_arc_decrement_strong_count_ConcreteBaseAudioContext,
    rustArcDecrementStrongCountPtr: RustLib.instance.api
        .rust_arc_decrement_strong_count_ConcreteBaseAudioContextPtr,
  );

  Future<void> base() =>
      RustLib.instance.api.webAudioApiContextConcreteBaseAudioContextBase(
        that: this,
      );

  /// Unset the callback to run when the state of the AudioContext has changed
  Future<void> clearOnstatechange() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextClearOnstatechange(
        that: this,
      );

  /// Creates a `AnalyserNode`
  Future<AnalyserNode> createAnalyser() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateAnalyser(
        that: this,
      );

  /// Create an `AudioParam`.
  ///
  /// Call this inside the `register` closure when setting up your `AudioNode`
  Future<(AudioParam, AudioParamId)> createAudioParam(
          {required AudioParamDescriptor opts,
          required AudioContextRegistration dest}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateAudioParam(
              that: this, opts: opts, dest: dest);

  /// Creates an `BiquadFilterNode` which implements a second order filter
  Future<BiquadFilterNode> createBiquadFilter() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateBiquadFilter(
        that: this,
      );

  /// Create an new "in-memory" `AudioBuffer` with the given number of channels,
  /// length (i.e. number of samples per channel) and sample rate.
  ///
  /// Note: In most cases you will want the sample rate to match the current
  /// audio context sample rate.
  Future<AudioBuffer> createBuffer(
          {required BigInt numberOfChannels,
          required BigInt length,
          required double sampleRate}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateBuffer(
              that: this,
              numberOfChannels: numberOfChannels,
              length: length,
              sampleRate: sampleRate);

  /// Creates an `AudioBufferSourceNode`
  Future<AudioBufferSourceNode> createBufferSource() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateBufferSource(
        that: this,
      );

  /// Creates a `ChannelMergerNode`
  Future<ChannelMergerNode> createChannelMerger(
          {required BigInt numberOfInputs}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateChannelMerger(
              that: this, numberOfInputs: numberOfInputs);

  /// Creates a `ChannelSplitterNode`
  Future<ChannelSplitterNode> createChannelSplitter(
          {required BigInt numberOfOutputs}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateChannelSplitter(
              that: this, numberOfOutputs: numberOfOutputs);

  /// Creates an `ConstantSourceNode`, a source representing a constant value
  Future<ConstantSourceNode> createConstantSource() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateConstantSource(
        that: this,
      );

  /// Creates an `ConvolverNode`, a processing node which applies linear convolution
  Future<ConvolverNode> createConvolver() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateConvolver(
        that: this,
      );

  /// Creates a `DelayNode`, delaying the audio signal
  Future<DelayNode> createDelay({required double maxDelayTime}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateDelay(
              that: this, maxDelayTime: maxDelayTime);

  /// Creates a `DynamicsCompressorNode`, compressing the audio signal
  Future<DynamicsCompressorNode> createDynamicsCompressor() =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateDynamicsCompressor(
        that: this,
      );

  /// Creates an `GainNode`, to control audio volume
  Future<GainNode> createGain() =>
      RustLib.instance.api.webAudioApiContextConcreteBaseAudioContextCreateGain(
        that: this,
      );

  /// Creates an `IirFilterNode`
  ///
  /// # Arguments
  ///
  /// * `feedforward` - An array of the feedforward (numerator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  /// * `feedback` - An array of the feedback (denominator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  Future<IirFilterNode> createIirFilter(
          {required List<double> feedforward,
          required List<double> feedback}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateIirFilter(
              that: this, feedforward: feedforward, feedback: feedback);

  /// Creates an `OscillatorNode`, a source representing a periodic waveform.
  Future<OscillatorNode> createOscillator() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateOscillator(
        that: this,
      );

  /// Creates a `PannerNode`
  Future<PannerNode> createPanner() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreatePanner(
        that: this,
      );

  /// Creates a periodic wave
  ///
  /// Please note that this constructor deviates slightly from the spec by requiring a single
  /// argument with the periodic wave options.
  Future<PeriodicWave> createPeriodicWave(
          {required PeriodicWaveOptions options}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreatePeriodicWave(
              that: this, options: options);

  /// Creates an `ScriptProcessorNode` for custom audio processing (deprecated);
  ///
  /// # Panics
  ///
  /// This function panics if:
  /// - `buffer_size` is not 256, 512, 1024, 2048, 4096, 8192, or 16384
  /// - the number of input and output channels are both zero
  /// - either of the channel counts exceed [`crate::MAX_CHANNELS`]
  Future<ScriptProcessorNode> createScriptProcessor(
          {required BigInt bufferSize,
          required BigInt numberOfInputChannels,
          required BigInt numberOfOutputChannels}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateScriptProcessor(
              that: this,
              bufferSize: bufferSize,
              numberOfInputChannels: numberOfInputChannels,
              numberOfOutputChannels: numberOfOutputChannels);

  /// Creates an `StereoPannerNode` to pan a stereo output
  Future<StereoPannerNode> createStereoPanner() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateStereoPanner(
        that: this,
      );

  /// Creates a `WaveShaperNode`
  Future<WaveShaperNode> createWaveShaper() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCreateWaveShaper(
        that: this,
      );

  /// This is the time in seconds of the sample frame immediately following the last sample-frame
  /// in the block of audio most recently processed by the context’s rendering graph.
  Future<double> currentTime() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextCurrentTime(
        that: this,
      );

  /// Returns an `AudioDestinationNode` representing the final destination of all audio in the
  /// context. It can be thought of as the audio-rendering device.
  Future<AudioDestinationNode> destination() => RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextDestination(
        that: this,
      );

  /// Returns the `AudioListener` which is used for 3D spatialization
  Future<AudioListener> listener() =>
      RustLib.instance.api.webAudioApiContextConcreteBaseAudioContextListener(
        that: this,
      );

  /// Inform render thread that this node can act as a cycle breaker
  Future<void> markCycleBreaker({required AudioContextRegistration reg}) =>
      RustLib.instance.api
          .webAudioApiContextConcreteBaseAudioContextMarkCycleBreaker(
              that: this, reg: reg);

  /// The sample rate (in sample-frames per second) at which the `AudioContext` handles audio.
  Future<double> sampleRate() =>
      RustLib.instance.api.webAudioApiContextConcreteBaseAudioContextSampleRate(
        that: this,
      );

  /// Returns state of current context
  Future<AudioContextState> state() =>
      RustLib.instance.api.webAudioApiContextConcreteBaseAudioContextState(
        that: this,
      );
}

// Rust type: RustOpaqueMoi<flutter_rust_bridge::for_generated::RustAutoOpaqueInner<OfflineAudioContext>>
@sealed
class OfflineAudioContext extends RustOpaque {
  // Not to be used by end users
  OfflineAudioContext.frbInternalDcoDecode(List<dynamic> wire)
      : super.frbInternalDcoDecode(wire, _kStaticData);

  // Not to be used by end users
  OfflineAudioContext.frbInternalSseDecode(BigInt ptr, int externalSizeOnNative)
      : super.frbInternalSseDecode(ptr, externalSizeOnNative, _kStaticData);

  static final _kStaticData = RustArcStaticData(
    rustArcIncrementStrongCount: RustLib
        .instance.api.rust_arc_increment_strong_count_OfflineAudioContext,
    rustArcDecrementStrongCount: RustLib
        .instance.api.rust_arc_decrement_strong_count_OfflineAudioContext,
    rustArcDecrementStrongCountPtr: RustLib
        .instance.api.rust_arc_decrement_strong_count_OfflineAudioContextPtr,
  );

  Future<void> base() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextBase(
        that: this,
      );

  /// Unset the callback to run when the rendering has completed
  Future<void> clearOncomplete() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextClearOncomplete(
        that: this,
      );

  /// Unset the callback to run when the state of the AudioContext has changed
  Future<void> clearOnstatechange() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextClearOnstatechange(
        that: this,
      );

  /// Creates a `AnalyserNode`
  Future<AnalyserNode> createAnalyser() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateAnalyser(
        that: this,
      );

  /// Create an `AudioParam`.
  ///
  /// Call this inside the `register` closure when setting up your `AudioNode`
  Future<(AudioParam, AudioParamId)> createAudioParam(
          {required AudioParamDescriptor opts,
          required AudioContextRegistration dest}) =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateAudioParam(
              that: this, opts: opts, dest: dest);

  /// Creates an `BiquadFilterNode` which implements a second order filter
  Future<BiquadFilterNode> createBiquadFilter() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateBiquadFilter(
        that: this,
      );

  /// Create an new "in-memory" `AudioBuffer` with the given number of channels,
  /// length (i.e. number of samples per channel) and sample rate.
  ///
  /// Note: In most cases you will want the sample rate to match the current
  /// audio context sample rate.
  Future<AudioBuffer> createBuffer(
          {required BigInt numberOfChannels,
          required BigInt length,
          required double sampleRate}) =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateBuffer(
          that: this,
          numberOfChannels: numberOfChannels,
          length: length,
          sampleRate: sampleRate);

  /// Creates an `AudioBufferSourceNode`
  Future<AudioBufferSourceNode> createBufferSource() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateBufferSource(
        that: this,
      );

  /// Creates a `ChannelMergerNode`
  Future<ChannelMergerNode> createChannelMerger(
          {required BigInt numberOfInputs}) =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateChannelMerger(
              that: this, numberOfInputs: numberOfInputs);

  /// Creates a `ChannelSplitterNode`
  Future<ChannelSplitterNode> createChannelSplitter(
          {required BigInt numberOfOutputs}) =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateChannelSplitter(
              that: this, numberOfOutputs: numberOfOutputs);

  /// Creates an `ConstantSourceNode`, a source representing a constant value
  Future<ConstantSourceNode> createConstantSource() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateConstantSource(
        that: this,
      );

  /// Creates an `ConvolverNode`, a processing node which applies linear convolution
  Future<ConvolverNode> createConvolver() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateConvolver(
        that: this,
      );

  /// Creates a `DelayNode`, delaying the audio signal
  Future<DelayNode> createDelay({required double maxDelayTime}) =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateDelay(
          that: this, maxDelayTime: maxDelayTime);

  /// Creates a `DynamicsCompressorNode`, compressing the audio signal
  Future<DynamicsCompressorNode> createDynamicsCompressor() =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateDynamicsCompressor(
        that: this,
      );

  /// Creates an `GainNode`, to control audio volume
  Future<GainNode> createGain() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateGain(
        that: this,
      );

  /// Creates an `IirFilterNode`
  ///
  /// # Arguments
  ///
  /// * `feedforward` - An array of the feedforward (numerator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  /// * `feedback` - An array of the feedback (denominator) coefficients for the transfer function of the IIR filter.
  /// The maximum length of this array is 20
  Future<IirFilterNode> createIirFilter(
          {required List<double> feedforward,
          required List<double> feedback}) =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreateIirFilter(
          that: this, feedforward: feedforward, feedback: feedback);

  /// Creates an `OscillatorNode`, a source representing a periodic waveform.
  Future<OscillatorNode> createOscillator() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateOscillator(
        that: this,
      );

  /// Creates a `PannerNode`
  Future<PannerNode> createPanner() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCreatePanner(
        that: this,
      );

  /// Creates a periodic wave
  ///
  /// Please note that this constructor deviates slightly from the spec by requiring a single
  /// argument with the periodic wave options.
  Future<PeriodicWave> createPeriodicWave(
          {required PeriodicWaveOptions options}) =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreatePeriodicWave(
              that: this, options: options);

  /// Creates an `ScriptProcessorNode` for custom audio processing (deprecated);
  ///
  /// # Panics
  ///
  /// This function panics if:
  /// - `buffer_size` is not 256, 512, 1024, 2048, 4096, 8192, or 16384
  /// - the number of input and output channels are both zero
  /// - either of the channel counts exceed [`crate::MAX_CHANNELS`]
  Future<ScriptProcessorNode> createScriptProcessor(
          {required BigInt bufferSize,
          required BigInt numberOfInputChannels,
          required BigInt numberOfOutputChannels}) =>
      RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateScriptProcessor(
              that: this,
              bufferSize: bufferSize,
              numberOfInputChannels: numberOfInputChannels,
              numberOfOutputChannels: numberOfOutputChannels);

  /// Creates an `StereoPannerNode` to pan a stereo output
  Future<StereoPannerNode> createStereoPanner() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateStereoPanner(
        that: this,
      );

  /// Creates a `WaveShaperNode`
  Future<WaveShaperNode> createWaveShaper() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextCreateWaveShaper(
        that: this,
      );

  /// This is the time in seconds of the sample frame immediately following the last sample-frame
  /// in the block of audio most recently processed by the context’s rendering graph.
  Future<double> currentTime() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextCurrentTime(
        that: this,
      );

  /// Returns an `AudioDestinationNode` representing the final destination of all audio in the
  /// context. It can be thought of as the audio-rendering device.
  Future<AudioDestinationNode> destination() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextDestination(
        that: this,
      );

  /// get the length of rendering audio buffer
  Future<BigInt> length() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextLength(
        that: this,
      );

  /// Returns the `AudioListener` which is used for 3D spatialization
  Future<AudioListener> listener() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextListener(
        that: this,
      );

  // HINT: Make it `#[frb(sync)]` to let it become the default constructor of Dart class.
  /// Creates an `OfflineAudioContext` instance
  ///
  /// # Arguments
  ///
  /// * `channels` - number of output channels to render
  /// * `length` - length of the rendering audio buffer
  /// * `sample_rate` - output sample rate
  static Future<OfflineAudioContext> newInstance(
          {required BigInt numberOfChannels,
          required BigInt length,
          required double sampleRate}) =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextNew(
          numberOfChannels: numberOfChannels,
          length: length,
          sampleRate: sampleRate);

  /// Resumes the progression of the OfflineAudioContext's currentTime when it has been suspended
  ///
  /// # Panics
  ///
  /// Panics when the context is closed or rendering has not started
  Future<void> resume() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextResume(
        that: this,
      );

  /// The sample rate (in sample-frames per second) at which the `AudioContext` handles audio.
  Future<double> sampleRate() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextSampleRate(
        that: this,
      );

  /// Given the current connections and scheduled changes, starts rendering audio.
  ///
  /// Rendering is purely CPU bound and contains no `await` points, so calling this method will
  /// block the executor until completion or until the context is suspended.
  ///
  /// This method will only adhere to scheduled suspensions via [`Self::suspend`] and will
  /// ignore those provided via [`Self::suspend_sync`].
  ///
  /// # Panics
  ///
  /// Panics if this method is called multiple times.
  Future<AudioBuffer> startRendering() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextStartRendering(
        that: this,
      );

  /// Given the current connections and scheduled changes, starts rendering audio.
  ///
  /// This function will block the current thread and returns the rendered `AudioBuffer`
  /// synchronously.
  ///
  /// This method will only adhere to scheduled suspensions via [`Self::suspend_sync`] and
  /// will ignore those provided via [`Self::suspend`].
  ///
  /// # Panics
  ///
  /// Panics if this method is called multiple times
  Future<AudioBuffer> startRenderingSync() => RustLib.instance.api
          .webAudioApiContextOfflineAudioContextStartRenderingSync(
        that: this,
      );

  /// Returns state of current context
  Future<AudioContextState> state() =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextState(
        that: this,
      );

  /// Schedules a suspension of the time progression in the audio context at the specified time
  /// and returns a promise
  ///
  /// The specified time is quantized and rounded up to the render quantum size.
  ///
  /// # Panics
  ///
  /// Panics if the quantized frame number
  ///
  /// - is negative or
  /// - is less than or equal to the current time or
  /// - is greater than or equal to the total render duration or
  /// - is scheduled by another suspend for the same time
  ///
  /// # Example usage
  ///
  /// ```rust
  /// use futures::{executor, join};
  /// use futures::FutureExt as _;
  /// use std::sync::Arc;
  ///
  /// use web_audio_api::context::BaseAudioContext;
  /// use web_audio_api::context::OfflineAudioContext;
  /// use web_audio_api::node::{AudioNode, AudioScheduledSourceNode};
  ///
  /// let context = Arc::new(OfflineAudioContext::new(1, 512, 44_100.));
  /// let context_clone = Arc::clone(&context);
  ///
  /// let suspend_promise = context.suspend(128. / 44_100.).then(|_| async move {
  ///     let mut src = context_clone.create_constant_source();
  ///     src.connect(&context_clone.destination());
  ///     src.start();
  ///     context_clone.resume().await;
  /// });
  ///
  /// let render_promise = context.start_rendering();
  ///
  /// let buffer = executor::block_on(async move { join!(suspend_promise, render_promise).1 });
  /// assert_eq!(buffer.number_of_channels(), 1);
  /// assert_eq!(buffer.length(), 512);
  /// ```
  Future<void> suspend({required double suspendTime}) =>
      RustLib.instance.api.webAudioApiContextOfflineAudioContextSuspend(
          that: this, suspendTime: suspendTime);
}

@freezed
sealed class AudioContextLatencyCategory with _$AudioContextLatencyCategory {
  const AudioContextLatencyCategory._();

  /// Balance audio output latency and power consumption.
  const factory AudioContextLatencyCategory.balanced() =
      AudioContextLatencyCategory_Balanced;

  /// Provide the lowest audio output latency possible without glitching. This is the default.
  const factory AudioContextLatencyCategory.interactive() =
      AudioContextLatencyCategory_Interactive;

  /// Prioritize sustained playback without interruption over audio output latency.
  ///
  /// Lowest power consumption.
  const factory AudioContextLatencyCategory.playback() =
      AudioContextLatencyCategory_Playback;

  /// Specify the number of seconds of latency
  ///
  /// This latency is not guaranteed to be applied, it depends on the audio hardware capabilities
  const factory AudioContextLatencyCategory.custom(
    double field0,
  ) = AudioContextLatencyCategory_Custom;
}

/// Specify the playback configuration for the [`AudioContext`] constructor.
///
/// All fields are optional and will default to the value best suited for interactive playback on
/// your hardware configuration.
///
/// For future compatibility, it is best to construct a default implementation of this struct and
/// set the fields you would like to override:
/// ```
/// use web_audio_api::context::AudioContextOptions;
///
/// // Request a sample rate of 44.1 kHz, leave other fields to their default values
/// let opts = AudioContextOptions {
///     sample_rate: Some(44100.),
///     ..AudioContextOptions::default()
/// };
class AudioContextOptions {
  /// Identify the type of playback, which affects tradeoffs between audio output latency and
  /// power consumption.
  final AudioContextLatencyCategory latencyHint;

  /// Sample rate of the audio context and audio output hardware. Use `None` for a default value.
  final double? sampleRate;

  /// The audio output device
  /// - use `""` for the default audio output device
  /// - use `"none"` to process the audio graph without playing through an audio output device.
  /// - use `"sinkId"` to use the specified audio sink id, obtained with [`enumerate_devices_sync`]
  final String sinkId;

  /// Option to request a default, optimized or specific render quantum size. It is a hint that might not be honored.
  final AudioContextRenderSizeCategory renderSizeHint;

  const AudioContextOptions({
    required this.latencyHint,
    this.sampleRate,
    required this.sinkId,
    required this.renderSizeHint,
  });

  @override
  int get hashCode =>
      latencyHint.hashCode ^
      sampleRate.hashCode ^
      sinkId.hashCode ^
      renderSizeHint.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is AudioContextOptions &&
          runtimeType == other.runtimeType &&
          latencyHint == other.latencyHint &&
          sampleRate == other.sampleRate &&
          sinkId == other.sinkId &&
          renderSizeHint == other.renderSizeHint;
}

/// This allows users to ask for a particular render quantum size.
///
/// Currently, only the default value is available
enum AudioContextRenderSizeCategory {
  /// The default value of 128 frames
  Default,
  ;
}

/// Describes the current state of the `AudioContext`
enum AudioContextState {
  /// This context is currently suspended (context time is not proceeding,
  /// audio hardware may be powered down/released).
  suspended,

  /// Audio is being processed.
  running,

  /// This context has been released, and can no longer be used to process audio.
  /// All system audio resources have been released.
  closed,
  ;
}
